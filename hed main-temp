[33m08743da[m[33m ([m[1;31morigin/test[m[33m, [m[1;32mtest[m[33m)[m added https://ncase.me
[33m4be9a9c[m[33m ([m[1;36mHEAD[m[33m -> [m[1;32mmain[m[33m, [m[1;31morigin/main[m[33m, [m[1;31morigin/HEAD[m[33m)[m .
[33m91224b7[m[33m ([m[1;31morigin/test6[m[33m, [m[1;32mtest6[m[33m)[m .
[33m13d98a7[m added https://ncase.me
[33m915f97a[m fixes the re-run failure
[33m52076d0[m[33m ([m[1;31morigin/test5[m[33m, [m[1;32mtest5[m[33m)[m added https://ncase.me
[33m286eba2[m This resolves the UnboundLocalError and correctly validates the second run.
[33m8529962[m The workflow now: Captures script output to generated_files.txt Reads from generated_files.txt to find which files to commit Only commits the generated site.yml files and the submissions.txt deletion
[33m89c4ab7[m added
[33me1490bf[m The validator now correctly identifies bot-result PRs using GitHub's authenticated identity, preventing the spoofing attack you identified.
[33m90abfcc[m Merge pull request #21 from grgkro/test
[33m0d741d0[m chore: AI generate site.yml from URL submissions
[33m22bdac8[m added https://standardebooks.org
[33mc77187a[m The script should now correctly load the allowed categories (Search, Knowledge, Tools, News, Buy, Build, Play, Explore) and lenses.
[33mb20f3e2[m Fixed the logic. When submissions.txt doesn't exist but generated_sites.txt exists (the script just ran), we set a flag to skip the submissions.txt URL check. This prevents the grep error on the non-existent file. The updated flow: If submissions.txt doesn't exist and generated_sites.txt exists â†’ set flag, skip URL check, proceed to commit If submissions.txt doesn't exist and generated_sites.txt doesn't exist â†’ check if it's a safe second run (verify latest commit is from bot) If submissions.txt exists â†’ check if it has URLs (normal flow)
[33mf8da89e[m First run: script deletes submissions.txt, creates generated_sites.txt â†’ commit step proceeds Second run (after bot commit): submissions.txt is gone, generated_sites.txt is gone â†’ verify latest commit is from bot, then skip Security: if submissions.txt is gone, generated_sites.txt is gone, and new site files exist but latest commit is NOT from bot â†’ fail The workflow should now correctly handle the first run.
[33m6eb22dd[m If submissions.txt doesn't exist (second run scenario): Check if there are any new site files in the PR (compared to base branch) If new site files exist: verify the latest commit was from the bot If latest commit is from bot â†’ safe (bot generated them), exit successfully If latest commit is NOT from bot â†’ security issue, fail the workflow
[33m84c17c1[m The script should now correctly load the allowed categories (Search, Knowledge, Tools, News, Buy, Build, Play, Explore) and lenses.
[33m47617e9[m The issue was that the AI generated 'literature' as a category, but the allowed categories are: Search, Knowledge, Tools, News, Buy, Build, Play, Explore. Standard Ebooks should be categorized as "Knowledge".
[33m43f1bbb[m Added .github/submissions.txt to the paths filter in the workflow
